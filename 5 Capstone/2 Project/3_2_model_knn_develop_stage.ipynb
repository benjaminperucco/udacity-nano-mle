{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.session import get_execution_role\n",
    "from sagemaker.tuner import HyperparameterTuner, CategoricalParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default sagemaker parameters\n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print('Current SageMaker session: {}'.format(sagemaker_session))\n",
    "print('Current SageMaker default bucket: {}'.format(default_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload training data to s3 as csv without header and index\n",
    "data_path = ['train-tf-500-1', 'test-tf-500-1']\n",
    "input_data = []\n",
    "for specific_path in data_path:\n",
    "    tmp_path = sagemaker_session.upload_data(specific_path, bucket=default_bucket, key_prefix=specific_path)\n",
    "    input_data.append(tmp_path)\n",
    "\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit-learn estimator\n",
    "estimator = SKLearn(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    entry_point='train-knn.py', \n",
    "    source_dir='source', \n",
    "    framework_version='0.23-1', \n",
    "    py_version='py3', \n",
    "    hyperparameters={\n",
    "        'param_n_neighbors': 5,\n",
    "        'param_weight': 'uniform',\n",
    "        'param_p': 2\n",
    "    }   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "estimator.fit({'train': input_data[0], 'test': input_data[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "This acts as a baseline case for development of a tuning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range definition\n",
    "ranges = {\n",
    "    'param_n_neighbors': CategoricalParameter(list(np.arange(3, 13, 2))),\n",
    "    'param_weight': CategoricalParameter(['uniform', 'distance']),\n",
    "    'param_p': CategoricalParameter(list(np.arange(2, 8, 1)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric definition\n",
    "metrics = {\n",
    "    'Name': 'test-accuracy',\n",
    "    'Regex': 'test-accuracy: ([0-9\\\\.]+)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure hyperparameter tuning\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name='test-accuracy',\n",
    "    hyperparameter_ranges=ranges,\n",
    "    metric_definitions=[metrics],\n",
    "    max_parallel_jobs=3,\n",
    "    max_jobs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start hyperparameter tuning job\n",
    "tuner.fit({'train': input_data[0], 'test': input_data[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best training job model artifact\n",
    "best_model_data = 's3://{}/{}/output/model.tar.gz'.format(default_bucket, tuner.best_training_job())\n",
    "print(best_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit-learn model from training artifacts\n",
    "best_model = SKLearnModel(\n",
    "    model_data=best_model_data,\n",
    "    role=role,\n",
    "    entry_point='train-knn.py', \n",
    "    source_dir='source', \n",
    "    framework_version='0.23-1',\n",
    "    py_version='py3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy endpoint\n",
    "best_predictor = best_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read validation data\n",
    "validation_data = pd.read_csv('{}/{}'.format('validation-tf-500-1', 'validation.csv'))\n",
    "validation_y = validation_data.iloc[:, 0]\n",
    "validation_X = validation_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions based on test data\n",
    "best_pred_y = best_predictor.predict(validation_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics\n",
    "best_accuracy = accuracy_score(validation_y, best_pred_y)\n",
    "print('accuracy: {} %'.format(round(best_accuracy * 100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to test different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(dataset_list, base_hyperparameter_dict, sweep_hyperparameters_dict):\n",
    "    \n",
    "    print('define some sagemaker base parameters...', end='')\n",
    "    \n",
    "    # default sagemaker parameters\n",
    "    role = get_execution_role()\n",
    "    sagemaker_session = Session()\n",
    "    default_bucket = sagemaker_session.default_bucket()\n",
    "    \n",
    "    # metric definition\n",
    "    metrics = {\n",
    "        'Name': 'test-accuracy',\n",
    "        'Regex': 'test-accuracy: ([0-9\\\\.]+)'\n",
    "    }\n",
    "    \n",
    "    # create scikit-learn estimator\n",
    "    estimator = SKLearn(\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.c4.xlarge',\n",
    "        entry_point='train-knn.py', \n",
    "        source_dir='source', \n",
    "        framework_version='0.23-1', \n",
    "        py_version='py3', \n",
    "        hyperparameters=base_hyperparameter_dict\n",
    "    )\n",
    "    \n",
    "    # configure hyperparameter tuning\n",
    "    tuner = HyperparameterTuner(\n",
    "        estimator=estimator,\n",
    "        objective_metric_name='test-accuracy',\n",
    "        hyperparameter_ranges=sweep_hyperparameters_dict,\n",
    "        metric_definitions=[metrics],\n",
    "        max_parallel_jobs=3,\n",
    "        max_jobs=10\n",
    "    )\n",
    "    \n",
    "    print('done')\n",
    "    \n",
    "    accuracy_list = []\n",
    "    for dataset in dataset_list:\n",
    "        \n",
    "        train_data\n",
    "        \n",
    "    # upload data to s3, must be done in before any fitting happens\n",
    "    input_data = []\n",
    "    print('upload data to s3...', end='')\n",
    "    for dataset in dataset_list:\n",
    "        tmp_path = sagemaker_session.upload_data(specific_path, bucket=default_bucket, key_prefix=specific_path)\n",
    "        input_data.append(tmp_path)    \n",
    "    print('done')\n",
    "    \n",
    "        # start hyperparameter tuning job\n",
    "        tuner.fit({'train': input_data[0], 'test': input_data[1]})\n",
    "\n",
    "        # best training job model artifact\n",
    "        best_model_data = 's3://{}/{}/output/model.tar.gz'.format(default_bucket, tuner.best_training_job())\n",
    "\n",
    "        # create scikit-learn model from training artifacts\n",
    "        best_model = SKLearnModel(\n",
    "            model_data=best_model_data,\n",
    "            role=role,\n",
    "            entry_point='train-knn.py', \n",
    "            source_dir='source', \n",
    "            framework_version='0.23-1',\n",
    "            py_version='py3'\n",
    "        )\n",
    "\n",
    "        # deploy endpoint\n",
    "        best_predictor = best_model.deploy(\n",
    "            initial_instance_count=1, \n",
    "            instance_type='ml.t2.medium'\n",
    "        )\n",
    "\n",
    "        # read validation data\n",
    "        validation_data = pd.read_csv('{}/{}'.format('validation-tf-500-1', 'validation.csv'))\n",
    "        validation_y = validation_data.iloc[:, 0]\n",
    "        validation_X = validation_data.iloc[:, 1:]\n",
    "\n",
    "        # get metrics\n",
    "        best_accuracy = accuracy_score(validation_y, best_pred_y)\n",
    "        print('accuracy: {} %'.format(round(best_accuracy * 100, 1)\n",
    "                                      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
