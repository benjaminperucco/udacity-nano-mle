{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.session import get_execution_role\n",
    "from sagemaker.tuner import HyperparameterTuner, CategoricalParameter, IntegerParameter, ContinuousParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = image_uris.retrieve('xgboost', Session().boto_region_name, '1.2-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(s3_data='s3://{}/data/train-tf-44898-5000-1'.format(bucket), content_type='csv')\n",
    "s3_input_test = TrainingInput(s3_data='s3://{}/data/test-tf-44898-5000-1'.format(bucket), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 19:07:52 Starting - Starting the training job...\n",
      "2021-01-01 19:08:15 Starting - Launching requested ML instancesProfilerReport-1609528072: InProgress\n",
      "......\n",
      "2021-01-01 19:09:22 Starting - Preparing the instances for training............\n",
      "2021-01-01 19:11:17 Downloading - Downloading input data...\n",
      "2021-01-01 19:11:49 Training - Training image download completed. Training in progress..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 23379 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 7793 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.11010#011validation-error:0.11613\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.10193#011validation-error:0.10355\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.10060#011validation-error:0.10561\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.09393#011validation-error:0.09778\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.09094#011validation-error:0.09611\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.08820#011validation-error:0.08944\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.08495#011validation-error:0.08764\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.08140#011validation-error:0.08367\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.07550#011validation-error:0.07699\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.07246#011validation-error:0.07507\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.06878#011validation-error:0.07070\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.06561#011validation-error:0.06904\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.06343#011validation-error:0.06519\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.06065#011validation-error:0.06416\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.05800#011validation-error:0.06185\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.05672#011validation-error:0.05967\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.05381#011validation-error:0.05787\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.05120#011validation-error:0.05659\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.04773#011validation-error:0.05133\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.04555#011validation-error:0.05146\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.04363#011validation-error:0.04902\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.04269#011validation-error:0.04696\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.04140#011validation-error:0.04735\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.03944#011validation-error:0.04504\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.03832#011validation-error:0.04389\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.03674#011validation-error:0.04286\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.03644#011validation-error:0.04106\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.03469#011validation-error:0.04016\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.03430#011validation-error:0.03837\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.03366#011validation-error:0.03862\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.03264#011validation-error:0.03734\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.03217#011validation-error:0.03683\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.03122#011validation-error:0.03542\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.03075#011validation-error:0.03503\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.02998#011validation-error:0.03426\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.02913#011validation-error:0.03375\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.02866#011validation-error:0.03298\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.02810#011validation-error:0.03208\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.02738#011validation-error:0.03208\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.02682#011validation-error:0.03208\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.02613#011validation-error:0.03093\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.02554#011validation-error:0.03067\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.02511#011validation-error:0.03054\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.02502#011validation-error:0.02951\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.02425#011validation-error:0.02951\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.02387#011validation-error:0.02913\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.02335#011validation-error:0.02887\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.02331#011validation-error:0.02874\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.02340#011validation-error:0.02849\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.02267#011validation-error:0.02797\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.02246#011validation-error:0.02720\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.02211#011validation-error:0.02695\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.02169#011validation-error:0.02669\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.02087#011validation-error:0.02682\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.02083#011validation-error:0.02695\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.02036#011validation-error:0.02643\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.02028#011validation-error:0.02579\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.01997#011validation-error:0.02592\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.01938#011validation-error:0.02566\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.01908#011validation-error:0.02592\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.01920#011validation-error:0.02605\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.01843#011validation-error:0.02528\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.01814#011validation-error:0.02477\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.01796#011validation-error:0.02438\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.01792#011validation-error:0.02412\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.01779#011validation-error:0.02387\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.01732#011validation-error:0.02323\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.01719#011validation-error:0.02297\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.01677#011validation-error:0.02310\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.01681#011validation-error:0.02310\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.01647#011validation-error:0.02271\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.01677#011validation-error:0.02233\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.01660#011validation-error:0.02233\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.01630#011validation-error:0.02220\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.01583#011validation-error:0.02194\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.01557#011validation-error:0.02181\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.01548#011validation-error:0.02156\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.01519#011validation-error:0.02130\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.01519#011validation-error:0.02156\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.01489#011validation-error:0.02130\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.01437#011validation-error:0.02130\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.01450#011validation-error:0.02143\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.01429#011validation-error:0.02130\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.01407#011validation-error:0.02079\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.01403#011validation-error:0.02015\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.01399#011validation-error:0.02028\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.01377#011validation-error:0.02066\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.01339#011validation-error:0.02040\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.01317#011validation-error:0.02066\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.01322#011validation-error:0.02040\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.01313#011validation-error:0.02028\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.01296#011validation-error:0.02040\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.01296#011validation-error:0.02040\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.01279#011validation-error:0.02015\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.01245#011validation-error:0.02053\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.01219#011validation-error:0.01963\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.01223#011validation-error:0.01976\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.01215#011validation-error:0.02053\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.01202#011validation-error:0.02028\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.01185#011validation-error:0.02040\u001b[0m\n",
      "\n",
      "2021-01-01 19:15:20 Uploading - Uploading generated training model\n",
      "2021-01-01 19:15:20 Completed - Training job completed\n",
      "Training seconds: 261\n",
      "Billable seconds: 261\n"
     ]
    }
   ],
   "source": [
    "xgb = Estimator(container,\n",
    "                role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m4.xlarge')\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('s3://{}/data/validation-tf-44898-5000-1/validation.csv'.format(bucket))\n",
    "validation_y = validation_data.iloc[:, 0]\n",
    "validation_X = validation_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.array_split(validation_X, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=100):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(validation_X.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835750032080072"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_y, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9708712947517002"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_y, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.c5.2xlarge',\n",
    "                                    sagemaker_session=Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        rate_drop=0.3,\n",
    "                        tweedie_variance_power=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name = 'validation:rmse', # The metric used to compare trained models.\n",
    "                            objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                            hyperparameter_ranges=hyperparameter_ranges,\n",
    "                            max_jobs=8,\n",
    "                            max_parallel_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tuning(session, role, train_data, test_data, base_dict, sweep_dict, n_jobs=10, parallel_jobs=3):\n",
    "    \n",
    "    print(session)\n",
    "    \n",
    "    container = image_uris.retrieve('xgboost', session.boto_region_name, '1.2-1')\n",
    "    \n",
    "    print('here')\n",
    "    \n",
    "    regressor = sagemaker.estimator.Estimator(\n",
    "        container,\n",
    "        role, \n",
    "        instance_count=1, \n",
    "        instance_type='ml.c5.2xlarge',\n",
    "        sagemaker_session=session\n",
    "    ) \n",
    "    \n",
    "    regressor.set_hyperparameters(**base_dict)\n",
    "    \n",
    "    tuner = HyperparameterTuner(\n",
    "        regressor,\n",
    "        objective_metric_name='validation:rmse',\n",
    "        objective_type='Minimize',\n",
    "        hyperparameter_ranges=sweep_dict,\n",
    "        max_jobs=n_jobs,\n",
    "        max_parallel_jobs=parallel_jobs\n",
    "    )\n",
    "    \n",
    "    tuner.fit({'train': train_data, 'validation': test_data})\n",
    "    \n",
    "    #get_hyperparameters = tuner.hyperparameters()\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_best_model(tuner):\n",
    "    \n",
    "    predictor = tuner.deploy(initial_instance_count=1, instance_type='ml.t2.xlarge')\n",
    "    \n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_prediction(predictor, validation_data, rows=100):\n",
    "    \n",
    "    predictor.serializer = CSVSerializer()\n",
    "    \n",
    "    validation_y = validation_data.iloc[:, 0]\n",
    "    validation_X = validation_data.iloc[:, 1:].to_numpy()\n",
    "    \n",
    "    split_array = np.array_split(validation_X, int(validation_X.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=','), validation_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(model, data_prefix, dataset_list, base_dict, sweep_dict, n_jobs=10, parallel_jobs=3):\n",
    "    \n",
    "    print('define some SageMaker base parameters...', end='')\n",
    "        \n",
    "    sagemaker_session = Session()\n",
    "    role = get_execution_role()\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "    \n",
    "    print('done')\n",
    "    \n",
    "    for dataset in dataset_list:\n",
    "        \n",
    "        model_name = '{}-{}'.format(model, dataset)\n",
    "        print('evaluate model {}...'.format(model_name))\n",
    "        \n",
    "        # define input data\n",
    "        input_data = []\n",
    "        data_type_list = ['train', 'test', 'validation']\n",
    "        for data_type in data_type_list:\n",
    "            input_data.append('s3://{}/{}/{}-{}'.format(default_bucket, data_prefix, data_type, dataset))\n",
    "        \n",
    "        # configure hyperparameter tuning\n",
    "        s3_input_train = TrainingInput(s3_data=input_data[0], content_type='csv')\n",
    "        s3_input_test = TrainingInput(s3_data=input_data[1], content_type='csv')\n",
    "        \n",
    "        # start hyperparameter tuning job\n",
    "        print('start tuning', end='')\n",
    "        tuner, hyperparameters = perform_tuning(\n",
    "            session=sagemaker_session, \n",
    "            role=role, \n",
    "            train_data=s3_input_train, \n",
    "            test_data=s3_input_test,\n",
    "            base_dict=base_dict, \n",
    "            sweep_dict=sweep_dict, \n",
    "            n_jobs=n_jobs, \n",
    "            parallel_jobs=parallel_jobs\n",
    "        )\n",
    "\n",
    "        # deploy endpoint\n",
    "        print('deploy best model', end='')\n",
    "        predictor = deploy_best_model(tuner)\n",
    "        print('')\n",
    "\n",
    "        # read validation data\n",
    "        validation_data = pd.read_csv('{}/{}'.format(input_data[2], 'validation.csv'))\n",
    "        \n",
    "        # read test data\n",
    "        test_data = pd.read_csv('{}/{}'.format(input_data[1], 'test.csv'))\n",
    "        \n",
    "        # we have a lot of validation data, so we'll split it into batches of 100\n",
    "        # split the validate data set into batches and evaluate using prediction endpoint  \n",
    "        test_pred_y = perform_prediction(predictor, test_data, rows=100)\n",
    "        validation_pred_y = perform_prediction(predictor, test_data, rows=100)              \n",
    "\n",
    "        # get accuracy metrics\n",
    "        test_accuracy = accuracy_score(test_y, test_pred_y)\n",
    "        print('test model...accuracy: {} %'.format(round(test_accuracy * 100, 1)))\n",
    "        validation_accuracy = accuracy_score(validation_y, validation_pred_y)\n",
    "        print('validate model...accuracy: {} %'.format(round(validation_accuracy * 100, 1)))\n",
    "        \n",
    "        # save model results continuously into files in case something crashes, we have at least old results\n",
    "        validation_prefix = 'validation-{}'.format(model) # add prefix where data will be stored\n",
    "        tuned_model_prefix = 'tuned-model-{}'.format(model) # add prefix where best model data will be stored\n",
    "        save_model_results(validation_accuracy, test_accuracy, dataset, validation_prefix)\n",
    "        save_best_model_parameters(best_model_parameters, dataset, tuned_model_prefix)\n",
    "        \n",
    "        # remove resources\n",
    "        best_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data('knn', 'data', data, base, ranges, jobs, parallel_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dict = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'num_round': 100,\n",
    "    'rate_drop':0.3,\n",
    "    'tweedie_variance_power': 1.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_dict = {\n",
    "    'eta': ContinuousParameter(0, 1),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(s3_data='s3://{}/data/train-tf-44898-5000-1'.format(bucket), content_type='csv')\n",
    "s3_input_test = TrainingInput(s3_data='s3://{}/data/test-tf-44898-5000-1'.format(bucket), content_type='csv')\n",
    "validation_data = pd.read_csv('s3://{}/data/validation-tf-44898-5000-1/validation.csv'.format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.session.Session object at 0x7ff0e1f23710>\n",
      "here\n",
      "...........................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner = perform_tuning(sagemaker_session, role, s3_input_train, s3_input_test, base_dict, sweep_dict, n_jobs=3, parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-01 21:40:47 Starting - Preparing the instances for training\n",
      "2021-01-01 21:40:47 Downloading - Downloading input data\n",
      "2021-01-01 21:40:47 Training - Training image download completed. Training in progress.\n",
      "2021-01-01 21:40:47 Uploading - Uploading generated training model\n",
      "2021-01-01 21:40:47 Completed - Training job completed\n",
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = deploy_best_model(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = perform_prediction(predictor, validation_data, rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = result.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982676761195945"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_y, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.56565535e-04, 7.88106769e-03, 9.99999285e-01, ...,\n",
       "       6.60055521e-05, 2.08484598e-05, 1.96906467e-06])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
