{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing\n",
    "\n",
    "Postprocessing generates a result table of achieved accuracies on the validation and test data and relates them to the model, corpus size, feature size and n-gram size used (see variable <code>results</code>).\n",
    "\n",
    "- [Complete output of all tested models](#Complete-output-of-all-tested-models)\n",
    "\n",
    "A detailed output of each model is also generated.\n",
    "\n",
    "- [k nearest neighbors models](#k-nearest-neighbors-models)\n",
    "- [Support vector models](#Support-vector-models)\n",
    "- [Logistic regression models](#Logistic-regression-models)\n",
    "- [Gradient boosting models](#Gradient-boosting-models)\n",
    "- [Recurrent neural network models](#Recurrent-neural-network-models)\n",
    "\n",
    "Furthermore, all variables that influence the performance of the machine learning models are plotted agains each other.\n",
    "\n",
    "- [Model performance](#Model-performance)\n",
    "- [Feature number performance](#Feature-number-performance)\n",
    "- [Feature method performance](#Feature-method-performance)\n",
    "- [n-gram performance](#n-gram-performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read validation data\n",
    "\n",
    "Validation data is generated by the machine learning training and stored on the local file system in folders called <code>validation-</code> and followed by a model identifier (for example <code>log</code> for a logistic regression model). The files itself include the feature method (like <code>tf</code> for term frequency or <code>tf-idf</code> for term frequency inverse document frequency), the corpus size, the feature size and the n-gram size. This is then written as <code>tf-5000-500-1</code> for example, which means a term frequency method with a corpus size of 5000 and a feature size of 500 based on a unigram. The file further included both accuracies achieved during hyperparameter tuning based on the test data and the final accuracy achieved on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validation_directory():\n",
    "    \"\"\"\n",
    "    Lists all validation directories on the local file system starting \n",
    "    with 'validation-' and followed by the model identifier.\n",
    "    \"\"\"\n",
    "    available_content = os.listdir('.') \n",
    "    select_pattern = re.compile('^validation-[a-z][a-z][a-z]')\n",
    "    selected_dir = [content for content in available_content if select_pattern.match(content)]\n",
    "    return selected_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validation_directory_content(validation_directories):\n",
    "    \"\"\"\n",
    "    Reads the content from all files in the validation folders read by read_validation_directory().\n",
    "    \n",
    "    Args:\n",
    "    - validation_directories (list of str): List of validation directories.\n",
    "    \n",
    "    Returns:\n",
    "    - imported_data (pandas dataframe): Merged dataframe holding all the information from the files.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    for directory in validation_directories:\n",
    "        available_content = os.listdir(directory)\n",
    "        for file in available_content:\n",
    "            if iteration == 0:\n",
    "                imported_data = pd.read_csv('{}/{}'.format(directory, file))\n",
    "                imported_data['model'] = directory[11:]\n",
    "            else:\n",
    "                tmp_data = pd.read_csv('{}/{}'.format(directory, file)) \n",
    "                tmp_data['model'] = directory[11:] \n",
    "                imported_data = pd.concat([imported_data, tmp_data])\n",
    "            iteration += 1\n",
    "    \n",
    "    imported_data.reset_index(drop=True, inplace=True)\n",
    "    return imported_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(x):\n",
    "    \"\"\"\n",
    "    Performs a text split between '-' character.\n",
    "    \n",
    "    Args:\n",
    "    - x (str): Text to be splitted.\n",
    "    \n",
    "    Returns:\n",
    "    - y (list of str): List of splitted text.\n",
    "    \"\"\" \n",
    "    y = x.split('-')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_information(df):\n",
    "    \"\"\"\n",
    "    Convert the abbreviated information of feature method, corpus size, feature size and n-gram size to columns.\n",
    "    \n",
    "    Args:\n",
    "    - df (pandas dataframe): Pandas dataframe to make the conversion on. The information to be splitted must be in \n",
    "    columnn 'data'.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas dataframe): Converted dataframe.\n",
    "    \"\"\"\n",
    "    df_splitted = df['data'].apply(text_split)\n",
    "    \n",
    "    feature_method_list = []\n",
    "    document_size_list = []\n",
    "    feature_size_list = []\n",
    "    ngram_size_list = []\n",
    "    \n",
    "    for splitted_entries in df_splitted:\n",
    "        if splitted_entries[1] == 'idf': # inverse document frequency case\n",
    "            feature_method_list.append('tf-idf')\n",
    "            document_size_list.append(splitted_entries[2])\n",
    "            feature_size_list.append(splitted_entries[3])\n",
    "            ngram_size_list.append(splitted_entries[4])\n",
    "        else:\n",
    "            feature_method_list.append('tf')\n",
    "            document_size_list.append(splitted_entries[1])\n",
    "            feature_size_list.append(splitted_entries[2])\n",
    "            ngram_size_list.append(splitted_entries[3])\n",
    "    \n",
    "    df['method'] = feature_method_list\n",
    "    df['corpus'] = document_size_list\n",
    "    df['feature'] = feature_size_list\n",
    "    df['ngram'] = ngram_size_list\n",
    "    df.drop(['data'], axis=1, inplace=True)\n",
    "    \n",
    "    # set index\n",
    "    df.index = df['model'] + '-' + df['method'] + '-' + df['corpus'] + '-' + df['feature'] + '-' + df['ngram']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_percent(x):\n",
    "    \"\"\"\n",
    "    Converts accuracy values (e.g. from 0.9633 to 96.33).\n",
    "    \n",
    "    Args:\n",
    "    - x (float): Value to be converted.\n",
    "    \n",
    "    Returns:\n",
    "    - y (float): Converted value.\n",
    "    \"\"\"\n",
    "    y = round(x * 100, 2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results():\n",
    "    \"\"\"\n",
    "    Read complete results of all tested models.\n",
    "    \"\"\"\n",
    "    validation_directories = read_validation_directory()\n",
    "    raw_results = read_validation_directory_content(validation_directories)\n",
    "    results = extract_feature_information(raw_results)\n",
    "    results.sort_values(by=['validation_accuracy'], ascending=False, inplace=True)\n",
    "    results['validation_accuracy'] = to_percent(results['validation_accuracy'])\n",
    "    results['test_accuracy'] = to_percent(results['test_accuracy'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete output of all tested models\n",
    "\n",
    "The variable <code>results</code> contains all tested combinations of model, feature method, corpus size, feature size and n-gram size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from all validation directories the accuracy and determine model settings\n",
    "results = read_results()\n",
    "results.head(results.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tuned models\n",
    "\n",
    "In order to see the hyperparameters for each model in detail for each setting, the following subchapters include this intersting information. The model details are stored in a folder on the file system beginning with <code>tuned-model-</code> followed again by a model identifier. In the file, the model parameters that produce the best accuracy on the test data are stored (after hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tuned_model_directory_content(prefix):\n",
    "    \"\"\"\n",
    "    Low level file reader that converts content into pandas dataframe.\n",
    "    \n",
    "    Args:\n",
    "    - prefix (str): Denoting the identifier for the machine learning model.\n",
    "    \n",
    "    Returns:\n",
    "    - imported_data (pandas dataframe): Imported pandas dataframe.\n",
    "    \"\"\"\n",
    "    path = 'tuned-model-{}'.format(prefix)\n",
    "    available_content = os.listdir(path)\n",
    "    iteration = 0\n",
    "    for file in available_content:\n",
    "        if iteration == 0:\n",
    "            imported_data = pd.read_csv('{}/{}'.format(path, file))\n",
    "            imported_data['model'] = prefix\n",
    "        else:\n",
    "            tmp_data = pd.read_csv('{}/{}'.format(path, file)) \n",
    "            tmp_data['model'] = prefix\n",
    "            imported_data = pd.concat([imported_data, tmp_data])\n",
    "        iteration += 1   \n",
    "            \n",
    "    imported_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return imported_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tuned_model_results(prefix):\n",
    "    \"\"\"\n",
    "    Reads the tuned model results from hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "    - prefix (str): Denoting the identifier for the machine learning model.\n",
    "    \n",
    "    Returns:\n",
    "    - results (pandas dataframe): Resulting hyperparameters.\n",
    "    \"\"\"\n",
    "    raw_results = read_tuned_model_directory_content(prefix)\n",
    "    results = extract_feature_information(raw_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_accuracy(tuned_model_results, accuracy_results):\n",
    "    \"\"\"\n",
    "    Joins (inner) the detailed model results and the general output of all model results together based on the index.\n",
    "    \n",
    "    Args:\n",
    "    - tuned_model_results (pandas dataframe): Dataframe holding the tuned model results \n",
    "    (obtained from function read_tuned_model_results()).\n",
    "    - accuracy_results (pandas dataframe): Dataframe holding the complete accuracy results of all models and influential\n",
    "    variables.\n",
    "    \n",
    "    Returns:\n",
    "    - tuned_model_results (pandas dataframe): Dataframe containing the join of both tables. \n",
    "    \"\"\"\n",
    "    # remove features present in both dataframes\n",
    "    accuracy_drop = accuracy_results.drop(['model', 'method', 'corpus', 'feature', 'ngram'], axis=1)\n",
    "    \n",
    "    tuned_model_results = tuned_model_results.join(accuracy_drop, how='inner')\n",
    "    tuned_model_results.sort_values(by=['validation_accuracy'], ascending=False, inplace=True)\n",
    "    tuned_model_results.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return tuned_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb = read_tuned_model_results('xgb')\n",
    "results_xgb = join_with_accuracy(results_xgb, results)\n",
    "results_xgb.head(results_xgb.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k nearest neighbors models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_knn = read_tuned_model_results('knn')\n",
    "#results_knn = join_with_accuracy(results_knn, results)\n",
    "#results_knn.head(results_knn.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_svc = read_tuned_model_results('svc')\n",
    "#results_svc = join_with_accuracy(results_svc, results)\n",
    "#results_svc.head(results_svc.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_log = read_tuned_model_results('log')\n",
    "#results_log = join_with_accuracy(results_log, results)\n",
    "#results_log.head(results_log.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_gbc = read_tuned_model_results('gbc')\n",
    "#results_gbc = join_with_accuracy(results_gbc, results)\n",
    "#results_gbc.head(results_gbc.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_mlp = read_tuned_model_results('mlp')\n",
    "#results_mlp = join_with_accuracy(results_mlp, results)\n",
    "#results_mlp.head(results_mlp.shape[0]) # display all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance in detail\n",
    "\n",
    "We see here detailed boxplots about the features that influcence machine learning model training. We take all variations of feature method, feature size and n-gram size into account to make some general recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_average_performance(df, group_variable):\n",
    "    \"\"\"\n",
    "    Performs a mean calculation on the accuracy based on a grouped variable.\n",
    "    \n",
    "    Args:\n",
    "    - df (pandas dataframe): Dataframe to perform the grouping on.\n",
    "    - group_variable (str): Variable denoting the feature we will group on.\n",
    "    \n",
    "    Returns:\n",
    "    - performance (pandas dataframe): Grouped dataframe.\n",
    "    \"\"\"\n",
    "    performance = df[['validation_accuracy', group_variable]].groupby([group_variable]).mean()\n",
    "    performance.sort_values(by=['validation_accuracy'], ascending=False, inplace=True)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boxplot(df, x, path, output):\n",
    "    \"\"\"\n",
    "    Create boxplot.\n",
    "    \n",
    "    Args:\n",
    "    - x (list of str): Represents the classes on the x-axis.\n",
    "    - path (str): Output folder.\n",
    "    - output (str): Output file name.\n",
    "    \n",
    "    Returns:\n",
    "    - none: Saves a figure in the end on the file system {path}/{output}.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "    sns.set(font_scale = 2)\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(11.7, 8.27)\n",
    "    sns.boxplot(x=x, y='validation_accuracy', data=df, palette='Set1') \n",
    "    ax.set(xlabel=x, ylabel='validation accuracy (%)')\n",
    "    \n",
    "    fig.savefig('{}/{}'.format(path, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "\n",
    "Influence of the machine learning model on the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_performance = evaluate_average_performance(results, 'model')\n",
    "#model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_boxplot(results, 'model', 'output', 'model_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature number performance\n",
    "\n",
    "Influence of the feature size on the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_performance = evaluate_average_performance(results, 'feature')\n",
    "feature_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplot(results, 'feature', 'output', 'feature_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature method performance\n",
    "\n",
    "Influence of the feature method on the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_performance = evaluate_average_performance(results, 'method')\n",
    "method_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplot(results, 'method', 'output', 'method_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram performance\n",
    "\n",
    "Influence of the n-gram size on the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_performance = evaluate_average_performance(results, 'ngram')\n",
    "ngram_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplot(results, 'ngram', 'output', 'ngram_performance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
