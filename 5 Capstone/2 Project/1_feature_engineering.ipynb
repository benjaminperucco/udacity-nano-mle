{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Feature engineering is about conversion of documents (words) into numerical features for machine learning processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, training_fraction, test_fraction, validation_fraction):\n",
    "    \"\"\"\n",
    "    Split data into training, test and validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert training_fraction + test_fraction + validation_fraction == 1, 'split fraction do not add to 1'\n",
    "               \n",
    "    nrow = df.shape[0]\n",
    "    count_training = int(training_fraction * nrow)\n",
    "    count_test = int(test_fraction * nrow)\n",
    "    count_validation = int(validation_fraction * nrow)\n",
    "    \n",
    "    training_df = df[:count_training]\n",
    "    test_df = df[count_training:(count_test + count_training)]\n",
    "    validation_df = df[(count_test + count_training):]\n",
    "               \n",
    "    return training_df, test_df, validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(X):\n",
    "    \"\"\"\n",
    "    Calculate term frequency (TF). \n",
    "    \"\"\"\n",
    "    term_frequency_array = np.zeros(shape=X.shape)\n",
    "    iteration = 0\n",
    "    for array in X:\n",
    "        if array.sum() > 0:\n",
    "            term_frequency_array[iteration] = array / array.sum()\n",
    "        else:\n",
    "            term_frequency_array[iteration] = array\n",
    "        iteration += 1\n",
    "    return term_frequency_array        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_term_frequency():\n",
    "    \"\"\"\n",
    "    tbd.\n",
    "    \"\"\"\n",
    "    X = np.array([[4, 2, 7, 4, 9, 1, 0], \n",
    "                  [6, 3, 2, 0, 0, 8, 1]])\n",
    "    Y = calculate_term_frequency(X)\n",
    "    Y_result = np.array([[4/27, 2/27, 7/27, 4/27, 9/27, 1/27, 0/27], \n",
    "                         [6/20, 3/20, 2/20, 0/20, 0/20, 8/20, 1/20]])\n",
    "    comparison = Y == Y_result\n",
    "    equal_arrays = comparison.all()\n",
    "    assert equal_arrays == True, 'test_calculate_term_frequency: check of test failed'\n",
    "    print('all test_calculate_term_frequency tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_calculate_term_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inverse_document_frequency(term_frequency_matrix, nrow=None):\n",
    "    \"\"\"\n",
    "    Calculate inverse term frequency inverse document frequency (TF-IDF) based on term frequency matrix (TF).\n",
    "    \"\"\"\n",
    "    \n",
    "    # if nrow is given, initialize according to nrow, otherwise on shape of term_frequency_matrix\n",
    "    if nrow is None:\n",
    "        nrow = term_frequency_matrix.shape[0]\n",
    "\n",
    "    inverse_document_frequency_matrix = np.zeros(shape=(nrow, term_frequency_matrix.shape[1])) # initialize   \n",
    "    corpus_sum = term_frequency_matrix.copy() # matrix term_frequency is used later, therefore create a copy\n",
    "    \n",
    "    # count an n-gram only once per document\n",
    "    corpus_sum[corpus_sum > 0] = 1\n",
    "    \n",
    "    # inverse document frequency (idf) is the number of documents in the corpus (here X.shape[0]), \n",
    "    # divided by the number of documents where a word appears\n",
    "    idf_single_array = 1 + np.log(term_frequency_matrix.shape[0] / corpus_sum.sum(axis=0)) # sum over all documents\n",
    "\n",
    "    # for matrix matrix multiplication between term_freq and term_freq_inv_doc_freq elementwise\n",
    "    for i in range(nrow):\n",
    "        inverse_document_frequency_matrix[i] = idf_single_array\n",
    "    \n",
    "    return inverse_document_frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_inverse_document_frequency():\n",
    "    \"\"\"\n",
    "    tbd.\n",
    "    \"\"\"\n",
    "    X = np.array([[4, 2, 7, 4, 9, 1, 0], \n",
    "                  [6, 3, 2, 0, 0, 8, 1]])\n",
    "    X_tf = calculate_term_frequency(X)\n",
    "    Y = calculate_inverse_document_frequency(X_tf)\n",
    "    Y_result = np.array([[(1+np.log(1)), (1+np.log(1)), (1+np.log(1)), (1+np.log(2)), \n",
    "                          (1+np.log(2)), (1+np.log(1)), (1+np.log(2))], \n",
    "                         [(1+np.log(1)), (1+np.log(1)), (1+np.log(1)), \n",
    "                          (1+np.log(2)), (1+np.log(2)), (1+np.log(1)), (1+np.log(2))]])\n",
    "    comparison = Y == Y_result\n",
    "    equal_arrays = comparison.all()\n",
    "    assert equal_arrays == True, 'test_calculate_inverse_document_frequency: check of test failed'\n",
    "    print('all test_calculate_inverse_document_frequency tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_calculate_inverse_document_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(df, feature_size, n_gram_size):\n",
    "    \"\"\"\n",
    "    tbd.\n",
    "    \"\"\"\n",
    "    corpus = df['processed_text'].values\n",
    "    vectorizer = CountVectorizer(max_features=feature_size, ngram_range=(n_gram_size, n_gram_size))\n",
    "    vectorizer.fit(corpus) # use training data to generate dictionary\n",
    "    feature_dictionary = vectorizer.get_feature_names() # feature order\n",
    "    return vectorizer, feature_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(train_df, test_df, validation_df, feature_size, n_gram_size):\n",
    "    \"\"\"\n",
    "    Build the feature matrices necessary for machine learning. \n",
    "    \n",
    "    Args:\n",
    "    - train_df (dataframe): Training document corpus with processed texts saved as feature 'processed_text' and its \n",
    "    corresponding class label available as 'class'.\n",
    "    - test_df (dataframe): Test document corpus.\n",
    "    - validation_df (dataframe): Validation document corpus.\n",
    "    - feature_size: (int) Defines the number of top n-grams extracted from the corpus, this defines the number of columns\n",
    "    of the feature matrix.\n",
    "    - n_gram_size: (int) Defines the n-gram size (contiguous sequence of items with length n).\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    \n",
    "    # build dictionary just on training data\n",
    "    transformer, dictionary = build_dictionary(train_df, feature_size, n_gram_size)\n",
    "\n",
    "    # build feature matrices for all partial datasets\n",
    "    train_matrix = transformer.transform(train_df['processed_text'].values).toarray()\n",
    "    test_matrix = transformer.transform(test_df['processed_text'].values).toarray()\n",
    "    validation_matrix = transformer.transform(validation_df['processed_text'].values).toarray()\n",
    "    \n",
    "    # term frequency (TF) can be calculated for all partial datasets individually\n",
    "    tf_train = calculate_term_frequency(train_matrix)\n",
    "    tf_test = calculate_term_frequency(test_matrix)\n",
    "    tf_validation = calculate_term_frequency(validation_matrix)\n",
    "    \n",
    "    # inverse document frequency (IDF), we can take just training data\n",
    "    idf_train = calculate_inverse_document_frequency(tf_train)\n",
    "    idf_test = calculate_inverse_document_frequency(tf_train, tf_test.shape[0])\n",
    "    idf_validation = calculate_inverse_document_frequency(tf_train, tf_validation.shape[0])\n",
    "    \n",
    "    # calculate term frequency inverse document frequency (TF-IDF)\n",
    "    tf_idf_train = np.multiply(tf_train, idf_train)\n",
    "    tf_idf_test = np.multiply(tf_test, idf_test)\n",
    "    tf_idf_validation = np.multiply(tf_validation, idf_validation)\n",
    "\n",
    "    return tf_train, tf_idf_train, tf_test, tf_idf_test, tf_validation, tf_idf_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(y, X, session, bucket, prefix, feature_type, data_type, corpus_size, n_gram_size):\n",
    "    \"\"\"\n",
    "    tbd.\n",
    "    \"\"\"\n",
    "     \n",
    "    feature_size = X.shape[1]\n",
    "    \n",
    "    print('upload {}-{}-{}-{}-{}'.format(data_type, feature_type, corpus_size, feature_size, n_gram_size))\n",
    "    \n",
    "    # tmp folder is used for local file creation and then from there upload to s3\n",
    "    if not os.path.exists('tmp'):\n",
    "        os.mkdir('tmp')\n",
    "    \n",
    "    df = pd.concat([pd.DataFrame(y), pd.DataFrame(X)], axis=1)\n",
    "    data_path = '{}/{}-{}-{}-{}-{}'.format('tmp', data_type, feature_type, corpus_size, feature_size, n_gram_size) \n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "        \n",
    "    df.to_csv('{}/{}.csv'.format(data_path, data_type), index=False, header=False)\n",
    "    \n",
    "    # upload to s3\n",
    "    session.upload_data('tmp', bucket=bucket, key_prefix=prefix)\n",
    "    \n",
    "    # remove local files\n",
    "    !rm -rfd tmp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_df, test_df, validation_df, session, bucket, prefix, corpus_size, feature_size, n_gram_size):\n",
    "    \"\"\"\n",
    "    tbd.\n",
    "    \"\"\"\n",
    "        \n",
    "    # build features\n",
    "    tf_train, tf_idf_train, tf_test, tf_idf_test, tf_validation, tf_idf_validation = \\\n",
    "        build_feature_matrix(train_df, test_df, validation_df, feature_size, n_gram_size)\n",
    "    \n",
    "    # store only class values\n",
    "    y_train = train_df['class'].values\n",
    "    y_test = test_df['class'].values\n",
    "    y_validation = validation_df['class'].values\n",
    "    \n",
    "    # upload to s3\n",
    "    upload_data(y_train, tf_train, session, bucket, prefix, 'tf', 'train', corpus_size, n_gram_size)\n",
    "    upload_data(y_train, tf_idf_train, session, bucket, prefix, 'tf-idf', 'train', corpus_size, n_gram_size)\n",
    "    upload_data(y_test, tf_test, session, bucket, prefix, 'tf', 'test', corpus_size, n_gram_size)\n",
    "    upload_data(y_test, tf_idf_test, session, bucket, prefix, 'tf-idf', 'test', corpus_size, n_gram_size)\n",
    "    upload_data(y_validation, tf_validation, session, bucket, prefix, 'tf', 'validation', corpus_size, n_gram_size)\n",
    "    upload_data(y_validation, tf_idf_validation, session, bucket, prefix, 'tf-idf', 'validation', corpus_size, n_gram_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to s3 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some SageMaker base parameters\n",
    "sagemaker_session = Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print('SageMaker session {}'.format(sagemaker_session))\n",
    "print('SageMaker default bucket {}'.format(default_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpus data\n",
    "corpus = pd.read_csv('{}/{}'.format('data', 'corpus-5000.csv'))\n",
    "corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split corpus data\n",
    "corpus_training, corpus_test, corpus_validation = split_data(corpus, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and upload to s3\n",
    "create_dataset(corpus_training, corpus_test, corpus_validation, sagemaker_session, default_bucket, 'data', 5000, 500, 1)\n",
    "create_dataset(corpus_training, corpus_test, corpus_validation, sagemaker_session, default_bucket, 'data', 5000, 1000, 1)\n",
    "create_dataset(corpus_training, corpus_test, corpus_validation, sagemaker_session, default_bucket, 'data', 5000, 5000, 1)\n",
    "create_dataset(corpus_training, corpus_test, corpus_validation, sagemaker_session, default_bucket, 'data', 5000, 500, 2)\n",
    "create_dataset(corpus_training, corpus_test, corpus_validation, sagemaker_session, default_bucket, 'data', 5000, 1000, 2)\n",
    "create_dataset(corpus_training, corpus_test, corpus_validation, sagemaker_session, default_bucket, 'data', 5000, 5000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
