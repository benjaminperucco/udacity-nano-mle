{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "data_path = 'data'\n",
    "data_set = 'clean_document.csv'\n",
    "term_frequency_data_set = 'term_frequency.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wto chief debat trump ralli support trade head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>catalonia find friend among eu leader european...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>shock fed govern grant disabl statu benefit sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>climat scammer al gore utterli embarrass expla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>lol leftist ca congresswoman tonight debat deb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                     processed_text\n",
       "0      0  wto chief debat trump ralli support trade head...\n",
       "1      0  catalonia find friend among eu leader european...\n",
       "2      1  shock fed govern grant disabl statu benefit sp...\n",
       "3      1  climat scammer al gore utterli embarrass expla...\n",
       "4      1  lol leftist ca congresswoman tonight debat deb..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_corpus = pd.read_csv('{}/{}'.format(data_path, data_set))\n",
    "complete_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(values, keys):\n",
    "    summed_values = values.sum(axis=0)\n",
    "    zipped_dictionary = zip(summed_values, keys)\n",
    "    sorted_zipped_dictionary = sorted(zipped_dictionary, reverse=True)\n",
    "    return sorted_zipped_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(X):\n",
    "    term_frequency_array = np.zeros(shape=X.shape)\n",
    "    iteration = 0\n",
    "    for array in X:\n",
    "        if array.sum() > 0:\n",
    "            term_frequency_array[iteration] = array / array.sum()\n",
    "        else:\n",
    "            term_frequency_array[iteration] = array\n",
    "        iteration += 1\n",
    "    return term_frequency_array        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency_inverse_document_frequency(X):\n",
    "    inverse_document_frequency = np.zeros(shape=X.shape)\n",
    "    term_frequency = calculate_term_frequency(X)\n",
    "    corpus_sum = term_frequency[term_frequency > 0] = 1 # count an n-gram only once per document\n",
    "    inverse_document_frequency_single_array = 1 + np.log(corpus_sum.sum(axis=0)) # sum over all documents\n",
    "    iteration = 0\n",
    "    for array in X:\n",
    "        inverse_document_frequency[iteration] = inverse_document_frequency_single_array\n",
    "        iteration += 1\n",
    "        \n",
    "    term_frequency_inverse_document_frequency = term_frequency * inverse_document_frequency\n",
    "    return term_frequency_inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(df, feature_size, n_gram_size):\n",
    "    corpus = df['processed_text'].values\n",
    "    vectorizer = CountVectorizer(max_features=feature_size, ngram_range=(n_gram_size, n_gram_size))\n",
    "    feature_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_vocabulary = vectorizer.get_feature_names() # feature order\n",
    "    return feature_vocabulary, feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(df, feature_size, n_gram_size):\n",
    "    \"\"\"\n",
    "    Build the features necessary for machine learning. \n",
    "    \n",
    "    Args:\n",
    "    df: (dataframe) Corpus with processed texts saved as feature 'processed_text' and its corresponding class \n",
    "    available as 'class'.\n",
    "    feature_size: (int) Defines the number of top n-grams extracted from the corpus.\n",
    "    n_gram_size: (int) Defines the n-gram size (contiguous sequence of items with length n).\n",
    "    \n",
    "    Returns:\n",
    "    feature_vocabulary: (list of n-grams) The vocabulary with the top feature_size n-grams.\n",
    "    feature_occurance: (numpy array) The occurance matrix of n-grams, number of rows = feature_size, \n",
    "    number of columns = corpus length.\n",
    "    feature_statistics: (dictionary) Dictionary with size feature_size sorted according to top n-gram.\n",
    "    term_freq: (numpy array) Term frequency matrix\n",
    "    term_freq_inverse_doc_freq: (numpy array) Maxrix according to term frequency inverse document frequency model.\n",
    "    \"\"\"\n",
    "    feature_vocabulary, feature_occurance = build_vocabulary(complete_corpus, feature_size, n_gram_size)\n",
    "    feature_statistics = count_features(feature_occurance, feature_vocabulary)\n",
    "    term_freq = calculate_term_frequency(feature_occurance)\n",
    "    term_freq_inverse_doc_freq = calculate_term_frequency_inverse_document_frequency(feature_occurance)\n",
    "    return feature_vocabulary, feature_occurance, feature_statistics, term_freq, term_freq_inverse_doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case\n",
    "\n",
    "Define a test case with 10 most used words in the corups and a n-gram of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-4c68030c29b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moccurance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_freq_inverse_doc_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-1282e3bd3f60>\u001b[0m in \u001b[0;36mbuild_feature\u001b[0;34m(df, feature_size, n_gram_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfeature_statistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_occurance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mterm_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_term_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_occurance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mterm_freq_inverse_doc_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_term_frequency_inverse_document_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_occurance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_occurance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_freq_inverse_doc_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-b75398d982ab>\u001b[0m in \u001b[0;36mcalculate_term_frequency_inverse_document_frequency\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_term_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcorpus_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm_frequency\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minverse_document_frequency_single_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "vocabulary, occurance, statistics, term_freq, term_freq_inverse_doc_freq = build_feature(complete_corpus, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clinton',\n",
       " 'one',\n",
       " 'peopl',\n",
       " 'presid',\n",
       " 'said',\n",
       " 'say',\n",
       " 'state',\n",
       " 'trump',\n",
       " 'would',\n",
       " 'year']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  3, ...,  4,  5,  4],\n",
       "       [ 0,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0,  2,  0, ...,  0,  1,  1],\n",
       "       ...,\n",
       "       [ 0,  0,  8, ...,  0,  1,  1],\n",
       "       [ 0,  1,  0, ..., 11,  0,  0],\n",
       "       [ 0,  3,  3, ...,  0,  3,  1]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(649, 'trump'),\n",
       " (586, 'said'),\n",
       " (272, 'would'),\n",
       " (270, 'state'),\n",
       " (221, 'presid'),\n",
       " (202, 'year'),\n",
       " (193, 'clinton'),\n",
       " (191, 'peopl'),\n",
       " (185, 'one'),\n",
       " (174, 'say')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.10714286, ..., 0.14285714, 0.17857143,\n",
       "        0.14285714],\n",
       "       [0.        , 0.125     , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.25      , 0.        , ..., 0.        , 0.125     ,\n",
       "        0.125     ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.44444444, ..., 0.        , 0.05555556,\n",
       "        0.05555556],\n",
       "       [0.        , 0.03448276, 0.        , ..., 0.37931034, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.11111111, 0.11111111, ..., 0.        , 0.11111111,\n",
       "        0.03703704]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_matrix = term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_matrix[idf_matrix > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = 1 + np.log(idf_matrix.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.09104245, 5.52178858, 5.40671925, 5.62497281, 6.03043792,\n",
       "       5.61512052, 5.59511985, 5.58496748, 5.56434819, 5.57471098])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build term frequency dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_term_frequency = pd.DataFrame(term_frequency, columns=feature_vocabulary)\n",
    "temp_term_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = pd.concat([document['class'], temp_term_frequency], axis=1)\n",
    "term_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_document(df, data_path, data_set):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv('{}/{}'.format(data_path, data_set), index=False)\n",
    "    print('{} saved'.format(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_document(term_frequency, data_path, term_frequency_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
