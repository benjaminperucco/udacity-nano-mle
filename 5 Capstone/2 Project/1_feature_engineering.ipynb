{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Feature engineering is about conversion of documents (words) into numerical features for machine learning processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nigerian presid say cannot afford return delta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>least four kill british motorway crash polic l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>exclus leap conclus trump drop one china polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>trump say black wors shape slaveri donald trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>vaniti fair leav melania best dress list look ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                     processed_text\n",
       "0      0  nigerian presid say cannot afford return delta...\n",
       "1      0  least four kill british motorway crash polic l...\n",
       "2      0  exclus leap conclus trump drop one china polic...\n",
       "3      1  trump say black wors shape slaveri donald trum...\n",
       "4      1  vaniti fair leav melania best dress list look ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('{}/{}'.format('data', 'corpus.csv'))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(values, keys):\n",
    "    summed_values = values.sum(axis=0)\n",
    "    zipped_dictionary = zip(summed_values, keys)\n",
    "    sorted_zipped_dictionary = sorted(zipped_dictionary, reverse=True)\n",
    "    return sorted_zipped_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(X):\n",
    "    \"\"\"\n",
    "    Calculate term frequency (TF). \n",
    "    \"\"\"\n",
    "    term_frequency_array = np.zeros(shape=X.shape)\n",
    "    iteration = 0\n",
    "    for array in X:\n",
    "        if array.sum() > 0:\n",
    "            term_frequency_array[iteration] = array / array.sum()\n",
    "        else:\n",
    "            term_frequency_array[iteration] = array\n",
    "        iteration += 1\n",
    "    return term_frequency_array        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_term_frequency():\n",
    "    X = np.array([[4, 2, 7, 4, 9, 1, 0], \n",
    "                  [6, 3, 2, 0, 0, 8, 1]])\n",
    "    Y = calculate_term_frequency(X)\n",
    "    Y_result = np.array([[4/27, 2/27, 7/27, 4/27, 9/27, 1/27, 0/27], \n",
    "                         [6/20, 3/20, 2/20, 0/20, 0/20, 8/20, 1/20]])\n",
    "    comparison = Y == Y_result\n",
    "    equal_arrays = comparison.all()\n",
    "    assert equal_arrays == True, 'test_calculate_term_frequency: check of test failed'\n",
    "    print('all test_calculate_term_frequency tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all test_calculate_term_frequency tests passed\n"
     ]
    }
   ],
   "source": [
    "test_calculate_term_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency_inverse_document_frequency(X):\n",
    "    \"\"\"\n",
    "    Calculate inverse term frequency inverse document frequency (TF-IDF).\n",
    "    \"\"\"\n",
    "    inverse_document_frequency = np.zeros(shape=X.shape)\n",
    "    term_frequency = calculate_term_frequency(X)\n",
    "    corpus_sum = term_frequency # matrix term_frequency is used later, therefore create a copy\n",
    "    \n",
    "    # count an n-gram only once per document\n",
    "    corpus_sum[corpus_sum > 0] = 1\n",
    "    \n",
    "    # inverse document frequency (idf) is the number of documents in the corpus (here X.shape[0]), \n",
    "    # divided by the number of documents where a word appears\n",
    "    idf_single_array = 1 + np.log(X.shape[0] / corpus_sum.sum(axis=0)) # sum over all documents\n",
    "\n",
    "    # for matrix matrix multiplication between term_freq and term_freq_inv_doc_freq elementwise\n",
    "    iteration = 0\n",
    "    for array in X:\n",
    "        inverse_document_frequency[iteration] = idf_single_array\n",
    "        iteration += 1\n",
    "        \n",
    "    term_frequency_inverse_document_frequency = np.multiply(term_frequency, inverse_document_frequency)\n",
    "    return term_frequency_inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(df, feature_size, n_gram_size):\n",
    "    corpus = df['processed_text'].values\n",
    "    vectorizer = CountVectorizer(max_features=feature_size, ngram_range=(n_gram_size, n_gram_size))\n",
    "    feature_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_vocabulary = vectorizer.get_feature_names() # feature order\n",
    "    return feature_vocabulary, feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(df, feature_size, n_gram_size):\n",
    "    \"\"\"\n",
    "    Build the features necessary for machine learning. \n",
    "    \n",
    "    Args:\n",
    "    - df: (dataframe) Corpus with processed texts saved as feature 'processed_text' and its corresponding class label \n",
    "    available as 'class'.\n",
    "    - feature_size: (int) Defines the number of top n-grams extracted from the corpus, this defines the number of columns\n",
    "    of the feature matrix.\n",
    "    - n_gram_size: (int) Defines the n-gram size (contiguous sequence of items with length n).\n",
    "    \n",
    "    Returns:\n",
    "    - feature_vocabulary: (list of n-grams) The vocabulary with the top feature_size n-grams.\n",
    "    - feature_occurance: (numpy array) The occurance matrix of n-grams, number of rows = feature_size, \n",
    "    number of columns = corpus length.\n",
    "    - feature_statistics: (dictionary) Dictionary with size feature_size sorted according to top n-gram.\n",
    "    - term_freq: (numpy array) Term frequency matrix.\n",
    "    - term_freq_inverse_doc_freq: (numpy array) Term frequency inverse document frequency maxrix.\n",
    "    \"\"\"\n",
    "    feature_vocabulary, feature_occurance = build_vocabulary(df, feature_size, n_gram_size)\n",
    "    feature_statistics = count_features(feature_occurance, feature_vocabulary)\n",
    "    term_freq = calculate_term_frequency(feature_occurance)\n",
    "    term_freq_inverse_doc_freq = calculate_term_frequency_inverse_document_frequency(feature_occurance)\n",
    "    return feature_vocabulary, feature_occurance, feature_statistics, term_freq, term_freq_inverse_doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case\n",
    "\n",
    "Define a test case with 10 most used words in the corups and a n-gram of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, occurance, statistics, term_freq, term_freq_inverse_doc_freq = build_feature(corpus, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15489, 'trump'),\n",
       " (14101, 'said'),\n",
       " (6996, 'state'),\n",
       " (6461, 'presid'),\n",
       " (5893, 'would'),\n",
       " (4578, 'year'),\n",
       " (4460, 'peopl'),\n",
       " (4333, 'republican'),\n",
       " (4282, 'say'),\n",
       " (4178, 'one')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_and_save(df, session, bucket, prefix, tf, tf_idf, number_documents, number_features, n_gram_size):\n",
    "    \"\"\"\n",
    "    Perform training, test and validation split. Training is used to train the model, test ist used for hyperparamter\n",
    "    tuning and validation is the final validation of the model. Splitting is done according to 60% training data, 20%\n",
    "    test data and 20% validation data.\n",
    "    \n",
    "    Returns:\n",
    "    Training, test or validation dataframe in its corresponding folder named as train-*, test-* or validation-*.\n",
    "    The first number defines the number of documents (rows), the second number defines the number of features (columns), \n",
    "    and the third number defines the n-gram.\n",
    "    \"\"\"\n",
    "\n",
    "    # tmp folder is used for local file creation and then from there upload to s3\n",
    "    if not os.path.exists('tmp'):\n",
    "        os.mkdir('tmp')\n",
    "    \n",
    "    # indicate all entries in path with all\n",
    "    if number_documents == 0:\n",
    "        number_documents = 'all'\n",
    "    \n",
    "    data_list = ['tf', 'tf-idf']\n",
    "    \n",
    "    for feature in data_list:\n",
    "        print('split and save {}-{}-{}-{}...'.format(feature, number_documents, number_features, n_gram_size), end='')\n",
    "        \n",
    "        if feature == 'tf':\n",
    "            X = tf # numpy\n",
    "        else:\n",
    "            X = tf_idf # numpy\n",
    "    \n",
    "        y = df['class'].values # numpy\n",
    "\n",
    "        # first split 60% as training data, rest 40% as test which will be split later 20% test, 20% validation\n",
    "        X_train, X_rest, y_train, y_rest = train_test_split(X, y, train_size=0.60, random_state=1)\n",
    "\n",
    "        # 50% / 50% split validation and test data\n",
    "        X_test, X_validation, y_test, y_validation = train_test_split(X_rest, y_rest, train_size=0.50, random_state=1)\n",
    "\n",
    "        data_path_train = \\\n",
    "            '{}/train-{}-{}-{}-{}'.format('tmp', feature, number_documents, number_features, n_gram_size) \n",
    "        data_path_test = \\\n",
    "            '{}/test-{}-{}-{}-{}'.format('tmp', feature, number_documents, number_features, n_gram_size) \n",
    "        data_path_validation = \\\n",
    "            '{}/validation-{}-{}-{}-{}'.format('tmp', feature, number_documents, number_features, n_gram_size) \n",
    "        data_path_list = [data_path_train, data_path_test, data_path_validation]\n",
    "\n",
    "        for check_path in data_path_list:\n",
    "            if not os.path.exists(check_path):\n",
    "                os.mkdir(check_path)\n",
    "\n",
    "        train_df = pd.concat([pd.DataFrame(y_train), pd.DataFrame(X_train)], axis=1)\n",
    "        test_df = pd.concat([pd.DataFrame(y_test), pd.DataFrame(X_test)], axis=1)\n",
    "        validation_df = pd.concat([pd.DataFrame(y_validation), pd.DataFrame(X_validation)], axis=1)\n",
    "\n",
    "        train_df.to_csv('{}/{}'.format(data_path_train, 'train.csv'), index=False, header=False)\n",
    "        test_df.to_csv('{}/{}'.format(data_path_test, 'test.csv'), index=False, header=False)\n",
    "        validation_df.to_csv('{}/{}'.format(data_path_validation, 'validation.csv'), index=False, header=False)\n",
    "        \n",
    "        # upload to s3\n",
    "        tmp_path = session.upload_data('tmp', bucket=bucket, key_prefix=prefix)\n",
    "        \n",
    "        print('done')\n",
    "        \n",
    "    print('remove local files...', end='')\n",
    "    \n",
    "    # remove local files\n",
    "    !rm -rfd tmp\n",
    "    \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_data(df, session, bucket, prefix, number_documents, number_features, n_gram_size):\n",
    "    \n",
    "    # define the number of documents analyzed, if 0 all data is used\n",
    "    if number_documents == 0:\n",
    "        corpus = df\n",
    "    else:\n",
    "        corpus = df.iloc[:number_documents]\n",
    "    \n",
    "    vocabulary, matrix, statistics, tf, tf_idf = build_feature(corpus, number_features, n_gram_size)\n",
    "    data_split_and_save(corpus, session, bucket, prefix, tf, tf_idf, number_documents, number_features, n_gram_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to s3 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker session <sagemaker.session.Session object at 0x7fbd8114db00>\n",
      "SageMaker default bucket sagemaker-us-east-1-385566775190\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print('SageMaker session {}'.format(sagemaker_session))\n",
    "print('SageMaker default bucket {}'.format(default_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split and save tf-5000-500-1...done\n",
      "split and save tf-idf-5000-500-1...done\n",
      "remove local files...done\n",
      "split and save tf-5000-1000-1...done\n",
      "split and save tf-idf-5000-1000-1...done\n",
      "remove local files...done\n",
      "split and save tf-5000-5000-1...done\n",
      "split and save tf-idf-5000-5000-1...done\n",
      "remove local files...done\n",
      "split and save tf-5000-500-2...done\n",
      "split and save tf-idf-5000-500-2...done\n",
      "remove local files...done\n",
      "split and save tf-5000-1000-2...done\n",
      "split and save tf-idf-5000-1000-2...done\n",
      "remove local files...done\n",
      "split and save tf-5000-5000-2...done\n",
      "split and save tf-idf-5000-5000-2...done\n",
      "remove local files...done\n"
     ]
    }
   ],
   "source": [
    "create_feature_data(corpus, sagemaker_session, default_bucket, 'data', 5000, 500, 1)\n",
    "create_feature_data(corpus, sagemaker_session, default_bucket, 'data', 5000, 1000, 1)\n",
    "create_feature_data(corpus, sagemaker_session, default_bucket, 'data', 5000, 5000, 1)\n",
    "create_feature_data(corpus, sagemaker_session, default_bucket, 'data', 5000, 500, 2)\n",
    "create_feature_data(corpus, sagemaker_session, default_bucket, 'data', 5000, 1000, 2)\n",
    "create_feature_data(corpus, sagemaker_session, default_bucket, 'data', 5000, 5000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
