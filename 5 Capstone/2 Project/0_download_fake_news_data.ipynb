{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download fake news data\n",
    "\n",
    "This notebook downloads the fake news data from kaggle and does some first analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# download required packages\n",
    "!pip install kaggle\n",
    "!pip install zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import unittest\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user parameters\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_data(data_path, data_set, extract_zip=True):\n",
    "    \n",
    "    print('Download {} from kaggle.com...'.format(data_set), end='')\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_file(\n",
    "        dataset='benjaminperucco/udacitydata',\n",
    "        file_name=data_set,\n",
    "        path=data_path,\n",
    "        force=True\n",
    "    )\n",
    "\n",
    "    file_path = '{}/{}.zip'.format(data_path, data_set)\n",
    "    \n",
    "    if extract_zip:\n",
    "        zf = ZipFile(file_path)\n",
    "        zf.extractall(data_path)\n",
    "        zf.close()\n",
    "        !rm $file_path\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download fake news data\n",
    "download_kaggle_data(data_path, 'Fake.csv')\n",
    "download_kaggle_data(data_path, 'True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kaggle_data(data_path, data_set):\n",
    "    \n",
    "    file_path = '{}/{}'.format(data_path, data_set)\n",
    "    if os.path.exists(file_path):\n",
    "        \n",
    "        print('Import {}...'.format(data_set), end='')\n",
    "    \n",
    "        file_path = '{}/{}'.format(data_path, data_set)\n",
    "        imported_data = pd.read_csv(file_path)\n",
    "        !rm $file_path\n",
    "    \n",
    "        print('done')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        empty_dict = {'dummy_col_1': [3, 2, 1, 0], 'dummy_col_2': ['a', 'b', 'c', 'd']} # just some test data\n",
    "        imported_data = pd.DataFrame.from_dict(empty_dict)\n",
    "        print('{} does not exist. Please download again kaggle data.'.format(file_path))\n",
    "    \n",
    "    return imported_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "fake = read_kaggle_data(data_path, 'Fake.csv')\n",
    "true = read_kaggle_data(data_path, 'True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class \n",
    "fake['class'] = 1 # 1 = fake\n",
    "true['class'] = 0 # 0 = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake[['title', 'text', 'subject', 'date']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true[['title', 'text', 'subject', 'date']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we have less unique values for title and text as counted values. This means several times the same titles and texts occur. I will remove not unique features once the text is processed further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data together and add classification\n",
    "original_document = pd.concat([true, fake])\n",
    "true = fake = None # to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_document[['title', 'text', 'subject', 'date']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprossed_text_investigation(index, df, number_strings=0):\n",
    "    assert number_strings >= 0, 'number_strings must be 0 or positive'   \n",
    "    \n",
    "    doc_class = df['class'].iloc[index]\n",
    "    title = df['title'].iloc[index]\n",
    "    text = df['text'].iloc[index]\n",
    "    if number_strings > 0:\n",
    "        text = text[:number_strings]\n",
    "        \n",
    "    print('INDEX {} - CLASS {} - {} {}\\n'.format(index, doc_class, title, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprossed_text_investigation(11010, original_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_from = 1\n",
    "select_to = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(select_from, select_to):\n",
    "    unprossed_text_investigation(i, original_document, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "\n",
    "First I will check what a regular text processing does, like removal of html tags, lower case but with no stemming. Title and text are put together for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following indices if text processing works reliable\n",
    "\n",
    "- 1112 (Trump twitter)\n",
    "- 284 (Trump twitter)\n",
    "- links (bit.ly/2jBh4LU) (bit.ly/2jpEXYR) bit.ly/2lnpKaq [1814 EST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text() # remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower()) # convert to lower case\n",
    "    words = text.split() # split string into words\n",
    "    words = [w for w in words if w not in stopwords.words('english')] # remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem \n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_us(text):\n",
    "    text = re.sub('[uU].[sS].', '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_us():\n",
    "    text_1 = 'The U.S. President or the u.s. President or even U.s. President'\n",
    "    result_text_1 = 'The  President or the  President or even  President'\n",
    "    assert remove_us(text_1) == result_text_1, 'test_remove_us: check of test 1 failed'\n",
    "    print('all test_remove_us tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_us()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_factbox(text):\n",
    "    text = re.sub('Factbox: Trump on Twitter', '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_intro(text):\n",
    "    text = re.sub('The following statements.*@realDonaldTrump', '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_reuters(text):\n",
    "    text = re.sub('.*\\(Reuters\\) -', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dates(text):\n",
    "    text = re.sub('[a-zA-Z]+ [0-9]?[0-9],? [0-9][0-9][0-9][0-9]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_dates():\n",
    "    text_1 = 'Shall we see each other March 5, 2017 again?'\n",
    "    result_text_1 = 'Shall we see each other  again?'\n",
    "    text_2 = 'Shall we see each other April 15, 2017 again?'\n",
    "    result_text_2 = 'Shall we see each other  again?'\n",
    "    text_3 = 'Shall we see each other May 5 2017 again?'\n",
    "    result_text_3 = 'Shall we see each other  again?'\n",
    "    text_4 = 'Shall we see each other January 17 again?'\n",
    "    result_text_4 = 'Shall we see each other January 17 again?'\n",
    "    assert remove_dates(text_1) == result_text_1, 'test_remove_dates: check of test 1 failed'\n",
    "    assert remove_dates(text_2) == result_text_2, 'test_remove_dates: check of test 2 failed'\n",
    "    assert remove_dates(text_3) == result_text_3, 'test_remove_dates: check of test 3 failed'\n",
    "    assert remove_dates(text_4) == result_text_4, 'test_remove_dates: check of test 4 failed'\n",
    "    print('all test_remove_dates tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_brackets():\n",
    "    text_1 = 'We will meet each other again April 20, 1980 and then we will have fun'\n",
    "    result_text_1 = 'We will meet each other again  and then we will have fun'\n",
    "    text_2 = 'We will meet each other again Dec 2, 1980 and then we will have fun'\n",
    "    result_text_2 = 'We will meet each other again  and then we will have fun'\n",
    "    text_3 = 'We will meet each other again Jan 17 and then we will have fun'\n",
    "    result_text_3 = 'We will meet each other again Jan 17 and then we will have fun'\n",
    "    text_4 = 'We will meet each other again Jan 17 1980 and then we will have fun'\n",
    "    result_text_4 = 'We will meet each other again  and then we will have fun'\n",
    "    assert remove_dates(text_1) == result_text_1, 'test_remove_brackets: check of test 1 failed'\n",
    "    assert remove_dates(text_2) == result_text_2, 'test_remove_brackets: check of test 2 failed'\n",
    "    assert remove_dates(text_3) == result_text_3, 'test_remove_brackets: check of test 3 failed'\n",
    "    assert remove_dates(text_4) == result_text_4, 'test_remove_brackets: check of test 4 failed'\n",
    "    print('all test_remove_brackets tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_brackets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(text):    \n",
    "    text = re.sub('[\\(\\[].*?[\\)\\]]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_brackets():\n",
    "    text_1 = '[VIDEO], [video], (other stuff), but not this'\n",
    "    result_text_1 = ', , , but not this'\n",
    "    assert remove_brackets(text_1) == result_text_1, 'test_remove_brackets: check of test 1 failed'\n",
    "    print('all test_remove_brackets tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_brackets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_source_link(text):\n",
    "    text = re.sub('[sS][oO][uU][rR][cC][eE] [lL][iI][nN][kK]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_source_link():\n",
    "    text_1 = 'All kind of source link or SOURCE LINK or Source Link or just link'\n",
    "    result_text_1 = 'All kind of  or  or  or just link'  \n",
    "    assert remove_source_link(text_1) == result_text_1, 'test_remove_source_link: check of test 1 failed'\n",
    "    print('all test_remove_source_link tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_source_link()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text): \n",
    "    text = re.sub('(?:\\\\s)[^\\\\s\\\\.]*\\\\.[^\\\\s]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_links():\n",
    "    text_1 = 'This text contains some tiny urls such as (bit.ly/2jBh4LU) and another (bit.ly/2jpEXYR) or also ' + \\\n",
    "             'normal URLs such as https://www.ubs.com/ch/de.html or http://www.ubs.com/ch/de.html'\n",
    "    result_text_1 = 'This text contains some tiny urls such as and another or also normal URLs such as or'  \n",
    "    text_2 = 'In another text, we use tiny urls without brackets like bit.ly/2jBh4LU and combining with longer ' + \\\n",
    "             'URLs like https://stackoverflow.com/questions/9043820/regex-to-match-words-of-a-certain-length ' + \\\n",
    "             'but it should not stop at the end but continue'\n",
    "    result_text_2 = 'In another text, we use tiny urls without brackets like and combining with longer ' + \\\n",
    "                    'URLs like but it should not stop at the end but continue'\n",
    "    text_3 = 'But is it also a problem when there is a slash at the end of the URL: https://stackoverflow.com/ ?'\n",
    "    result_text_3 = 'But is it also a problem when there is a slash at the end of the URL: ?'\n",
    "    assert remove_links(text_1) == result_text_1, 'test_remove_links: check of test 1 failed'\n",
    "    assert remove_links(text_2) == result_text_2, 'test_remove_links: check of test 2 failed'\n",
    "    assert remove_links(text_3) == result_text_3, 'test_remove_links: check of test 3 failed'\n",
    "    print('all test_remove_links tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remove_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(title, text):\n",
    "    text = remove_reuters(text) # reuters part needs to be removed first before title is added\n",
    "    text = '{} {}'.format(title, text)\n",
    "    text = remove_twitter_intro(text)\n",
    "    text = remove_twitter_factbox(text)\n",
    "    text = remove_dates(text)\n",
    "    text = remove_brackets(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_source_link(text)\n",
    "    text = remove_us(text)\n",
    "    text = clear_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prossed_text_investigation(index, df, number_strings=0):\n",
    "    \n",
    "    assert number_strings >= 0, 'number_strings must be 0 or positive' \n",
    "    \n",
    "    doc_class = df['class'].iloc[index]\n",
    "    title = df['title'].iloc[index]\n",
    "    text = df['text'].iloc[index]\n",
    "    text = text_processing(title, text)\n",
    "    if number_strings > 0:\n",
    "        text = text[:number_strings]\n",
    "        \n",
    "    print('INDEX {} - CLASS {} - {}\\n'.format(index, doc_class, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_index = 23876\n",
    "unprossed_text_investigation(check_index, original_document)\n",
    "prossed_text_investigation(check_index, original_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a clean dataset \n",
    "\n",
    "We also need to make sure to remove duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restruct_text(df):\n",
    "    nrow_data = len(df.index)\n",
    "    print('{} entries to process'.format(nrow_data))\n",
    "    iteration = 0\n",
    "    start_time = time.time()\n",
    "    for i in df.index:\n",
    "        title = df['title'].iloc[i]\n",
    "        text = df['text'].iloc[i]\n",
    "        iteration += 1\n",
    "        if iteration % 100 is 0:\n",
    "            time_now = time.time()\n",
    "            time_diff = time_now - start_time\n",
    "            total_time_estimated = nrow_data / iteration * time_diff\n",
    "            time_remaining = total_time_estimated - time_diff\n",
    "            print('Iterate {}/{} (time used: {:.0f}s, remaining time: {:.0f}s)'.\n",
    "                  format(iteration, nrow_data, time_diff, time_remaining))\n",
    "            \n",
    "        df.loc[i, 'processed_text'] = text_processing(title, text)\n",
    "        \n",
    "    print('done')\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the dataset to mix classes\n",
    "nrow_data = len(original_document.index)\n",
    "random.seed(4)\n",
    "sample_index = random.sample(range(nrow_data), nrow_data)\n",
    "original_document = original_document.iloc[sample_index]\n",
    "original_document.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to program the feature engineering, select only the top 200 entries from the sampled dataset\n",
    "sampled_document = original_document.iloc[:200].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start text processing\n",
    "processed_document = restruct_text(sampled_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_document(df, data_path, data_set):\n",
    "    \n",
    "    # keep only class and proc_text\n",
    "    df_save = df[['class', 'processed_text']].copy()\n",
    "    df_save.drop_duplicates(inplace=True)\n",
    "    df_save.to_csv('{}/{}'.format(data_path, data_set), index=False)\n",
    "    old_length = len(df.index)\n",
    "    new_length = len(df_save.index)\n",
    "    \n",
    "    \n",
    "    number_duplicates = (old_length - new_length) / old_length * 100\n",
    "    print('{} saved, data changed from {} to {}, approximately {:.2f}% duplicates'.\n",
    "          format(data_set, len(df.index), len(df_save.index), number_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_document(processed_document, 'data', 'clean_document.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
