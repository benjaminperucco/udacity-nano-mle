{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download fake news data\n",
    "\n",
    "This notebook downloads the fake news data from kaggle and performs text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install kaggle\n",
    "!pip install zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n",
    "import random\n",
    "import unittest\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_data(data_path, data_set, extract_zip):\n",
    "    \n",
    "    print('Download {} from kaggle.com...'.format(data_set), end='')\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_file\n",
    "        dataset='benjaminperucco/udacitydata',\n",
    "        file_name=data_set,\n",
    "        path=data_path,\n",
    "        force=True\n",
    "    )\n",
    "\n",
    "    file_path = '{}/{}.zip'.format(data_path, data_set)\n",
    "    \n",
    "    if extract_zip:\n",
    "        zf = ZipFile(file_path)\n",
    "        zf.extractall(data_path)\n",
    "        zf.close()\n",
    "        !rm $file_path\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kaggle_data(data_path, data_set):\n",
    "    \n",
    "    file_path = '{}/{}'.format(data_path, data_set)\n",
    "    if os.path.exists(file_path):\n",
    "        \n",
    "        print('Import {}...'.format(data_set), end='')\n",
    "    \n",
    "        file_path = '{}/{}'.format(data_path, data_set)\n",
    "        imported_data = pd.read_csv(file_path)\n",
    "        !rm $file_path\n",
    "    \n",
    "        print('done')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        empty_dict = {'dummy_col_1': [3, 2, 1, 0], 'dummy_col_2': ['a', 'b', 'c', 'd']} # just some test data\n",
    "        imported_data = pd.DataFrame.from_dict(empty_dict)\n",
    "        print('{} does not exist. Please download again kaggle data.'.format(file_path))\n",
    "    \n",
    "    return imported_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kaggle_data(data_path, extract_zip=True):\n",
    "    \n",
    "    # download fake news data\n",
    "    download_kaggle_data(data_path, 'Fake.csv', extract_zip)\n",
    "    download_kaggle_data(data_path, 'True.csv', extract_zip)\n",
    "    \n",
    "    # import data\n",
    "    fake = read_kaggle_data(data_path, 'Fake.csv')\n",
    "    true = read_kaggle_data(data_path, 'True.csv')\n",
    "    \n",
    "    # add class \n",
    "    fake['class'] = 1 # 1 = fake\n",
    "    true['class'] = 0 # 0 = true\n",
    "    \n",
    "    # merge data together and add classification\n",
    "    corpus = pd.concat([true, fake])\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = generate_kaggle_data('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about corpus\n",
    "\n",
    "We can see here that we have less unique values for title and text as counted values. This means several times the same titles and texts occur. I will remove not unique features once the text is processed further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[['title', 'text', 'subject', 'date']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "Title and text are put together for processing. Several processing steps are performed in the following order:\n",
    "\n",
    "- Remove everything that is before (Reuters) in the text. Mostly, these are cities where the article is referenced to.\n",
    "- Remove twitter intro. In case tweets from the U.S. President are truthful, these articles start with \"The following statements...\" and stop with @realDonaldTrump. Everything between is excluded.\n",
    "- Remove \"Factbox: Trump on Twitter\" intro.\n",
    "- Remove everything that looks like a date, for example March 20, 1989, Mar 20, 1989 or just Mar 20 1989. Dates contain no information regarding classifiction and are therefore removed.\n",
    "- Remove everything between brackets [] or (). Often links are between brackets, therefore we just remove them.\n",
    "- Remove everything that looks like a link.\n",
    "- Remove text with source link.\n",
    "- Remove U.S.\n",
    "- Further cleaning of text like removal of digits, html tags, stopwords and stemming is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    \"\"\"\n",
    "    Basic text processing:\n",
    "    - HTML tag removal\n",
    "    - Conversion to lower character\n",
    "    - Split words between spaces\n",
    "    - Stopwords removal\n",
    "    - Stem words\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text() # remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text.lower()) # convert to lower case\n",
    "    words = text.split() # split string into words\n",
    "    words = [w for w in words if w not in stopwords.words('english')] # remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem \n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_us(text):\n",
    "    \"\"\"\n",
    "    Remove <<U.S.>> from text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('[uU].[sS].', '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_us():\n",
    "    \"\"\"\n",
    "    Performs a test of function remove_us().\n",
    "    \"\"\"\n",
    "    text_1 = 'The U.S. President or the u.s. President or even U.s. President'\n",
    "    result_text_1 = 'The  President or the  President or even  President'\n",
    "    assert remove_us(text_1) == result_text_1, 'test_remove_us: check of test 1 failed'\n",
    "    print('all test_remove_us tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_factbox(text):\n",
    "    \"\"\"\n",
    "    Remove <<Factbox: Trump on Twitter>> from text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('Factbox: Trump on Twitter', '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_intro(text):\n",
    "    \"\"\"\n",
    "    Remove everything between <<The following statements>> and <<@realDonaldTrump>> from text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('The following statements.*@realDonaldTrump', '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_reuters(text):\n",
    "    \"\"\"\n",
    "    Remove everything before and including (Reuters) from text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('.*\\(Reuters\\) -', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dates(text):\n",
    "    \"\"\"\n",
    "    Remove dates written as Mar(ch) 20(,) 1982 from text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('[a-zA-Z]+ [0-9]?[0-9],? [0-9][0-9][0-9][0-9]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_dates():\n",
    "    \"\"\"\n",
    "    Performs a test of function remove_dates().\n",
    "    \"\"\"\n",
    "    text_1 = 'Shall we see each other March 5, 2017 again?'\n",
    "    result_text_1 = 'Shall we see each other  again?'\n",
    "    text_2 = 'Shall we see each other April 15, 2017 again?'\n",
    "    result_text_2 = 'Shall we see each other  again?'\n",
    "    text_3 = 'Shall we see each other May 5 2017 again?'\n",
    "    result_text_3 = 'Shall we see each other  again?'\n",
    "    text_4 = 'Shall we see each other January 17 again?'\n",
    "    result_text_4 = 'Shall we see each other January 17 again?'\n",
    "    assert remove_dates(text_1) == result_text_1, 'test_remove_dates: check of test 1 failed'\n",
    "    assert remove_dates(text_2) == result_text_2, 'test_remove_dates: check of test 2 failed'\n",
    "    assert remove_dates(text_3) == result_text_3, 'test_remove_dates: check of test 3 failed'\n",
    "    assert remove_dates(text_4) == result_text_4, 'test_remove_dates: check of test 4 failed'\n",
    "    print('all test_remove_dates tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(text):   \n",
    "    \"\"\"\n",
    "    Remove brackets and text between brackets like () or [].\n",
    "\n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('[\\(\\[].*?[\\)\\]]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_brackets():\n",
    "    \"\"\"\n",
    "    Performs a test of function remove_brackets().\n",
    "    \"\"\"\n",
    "    text_1 = '[VIDEO], [video], (other stuff), but not this'\n",
    "    result_text_1 = ', , , but not this'\n",
    "    assert remove_brackets(text_1) == result_text_1, 'test_remove_brackets: check of test 1 failed'\n",
    "    print('all test_remove_brackets tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_source_link(text):\n",
    "    \"\"\"\n",
    "    Remove words source link from text (written in any combination of lower or upper case).\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('[sS][oO][uU][rR][cC][eE] [lL][iI][nN][kK]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_source_link():\n",
    "    \"\"\"\n",
    "    Performs a test of function remove_source_link().\n",
    "    \"\"\"\n",
    "    text_1 = 'All kind of source link or SOURCE LINK or Source Link or just link'\n",
    "    result_text_1 = 'All kind of  or  or  or just link'  \n",
    "    assert remove_source_link(text_1) == result_text_1, 'test_remove_source_link: check of test 1 failed'\n",
    "    print('all test_remove_source_link tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text): \n",
    "    \"\"\"\n",
    "    Remove everything that looks like a link from text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Text to be cleared.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Cleared text.\n",
    "    \"\"\"\n",
    "    text = re.sub('(?:\\\\s)[^\\\\s\\\\.]*\\\\.[^\\\\s]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_links():\n",
    "    \"\"\"\n",
    "    Performs a test of function remove_links().\n",
    "    \"\"\"\n",
    "    text_1 = 'This text contains some tiny urls such as (bit.ly/2jBh4LU) and another (bit.ly/2jpEXYR) or also ' + \\\n",
    "             'normal URLs such as https://www.ubs.com/ch/de.html or http://www.ubs.com/ch/de.html'\n",
    "    result_text_1 = 'This text contains some tiny urls such as and another or also normal URLs such as or'  \n",
    "    text_2 = 'In another text, we use tiny urls without brackets like bit.ly/2jBh4LU and combining with longer ' + \\\n",
    "             'URLs like https://stackoverflow.com/questions/9043820/regex-to-match-words-of-a-certain-length ' + \\\n",
    "             'but it should not stop at the end but continue'\n",
    "    result_text_2 = 'In another text, we use tiny urls without brackets like and combining with longer ' + \\\n",
    "                    'URLs like but it should not stop at the end but continue'\n",
    "    text_3 = 'But is it also a problem when there is a slash at the end of the URL: https://stackoverflow.com/ ?'\n",
    "    result_text_3 = 'But is it also a problem when there is a slash at the end of the URL: ?'\n",
    "    assert remove_links(text_1) == result_text_1, 'test_remove_links: check of test 1 failed'\n",
    "    assert remove_links(text_2) == result_text_2, 'test_remove_links: check of test 2 failed'\n",
    "    assert remove_links(text_3) == result_text_3, 'test_remove_links: check of test 3 failed'\n",
    "    print('all test_remove_links tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(title, text):\n",
    "    \"\"\"\n",
    "    Execute text processing.\n",
    "    \n",
    "    Args:\n",
    "    - title (str): Title from news article.\n",
    "    - text (str): Text from news article.\n",
    "    \n",
    "    Returns:\n",
    "    - text (str): Processed text.\n",
    "    \"\"\"\n",
    "    text = remove_reuters(text) # reuters part needs to be removed first before title is added\n",
    "    text = '{} {}'.format(title, text)\n",
    "    text = remove_twitter_intro(text)\n",
    "    text = remove_twitter_factbox(text)\n",
    "    text = remove_dates(text)\n",
    "    text = remove_brackets(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_source_link(text)\n",
    "    text = remove_us(text)\n",
    "    text = clear_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prossed_text_investigation(index, df, number_strings=0):\n",
    "    assert number_strings >= 0, 'number_strings must be 0 or positive' \n",
    "    \n",
    "    doc_class = df['class'].iloc[index]\n",
    "    title = df['title'].iloc[index]\n",
    "    text = df['text'].iloc[index]\n",
    "    text = text_processing(title, text)\n",
    "    if number_strings > 0:\n",
    "        text = text[:number_strings]\n",
    "        \n",
    "    print('INDEX {} - CLASS {} - {}\\n'.format(index, doc_class, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprossed_text_investigation(index, df, number_strings=0):\n",
    "    assert number_strings >= 0, 'number_strings must be 0 or positive'   \n",
    "    \n",
    "    doc_class = df['class'].iloc[index]\n",
    "    title = df['title'].iloc[index]\n",
    "    text = df['text'].iloc[index]\n",
    "    if number_strings > 0:\n",
    "        text = text[:number_strings]\n",
    "        \n",
    "    print('INDEX {} - CLASS {} - {} {}\\n'.format(index, doc_class, title, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test of text processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_unit_tests():\n",
    "    test_remove_us()\n",
    "    test_remove_dates()\n",
    "    test_remove_brackets()\n",
    "    test_remove_source_link()\n",
    "    test_remove_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_unit_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate before and after text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_and_after_check(df, check_index):\n",
    "    unprossed_text_investigation(check_index, df)\n",
    "    prossed_text_investigation(check_index, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_and_after_check(corpus, 23876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a clean dataset \n",
    "\n",
    "We also need to make sure to remove duplicates! Furthermore, data resampling is performed for training, test and validation dataset extraction in a further step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restruct_text(df):\n",
    "    nrow_data = len(df.index)\n",
    "    print('{} entries to process'.format(nrow_data))\n",
    "    iteration = 0\n",
    "    start_time = time.time()\n",
    "    for i in df.index:\n",
    "        title = df['title'].iloc[i]\n",
    "        text = df['text'].iloc[i]\n",
    "        iteration += 1\n",
    "        if iteration % 100 is 0:\n",
    "            time_now = time.time()\n",
    "            time_diff = time_now - start_time\n",
    "            total_time_estimated = nrow_data / iteration * time_diff\n",
    "            time_remaining = total_time_estimated - time_diff\n",
    "            print('Iterate {}/{} (time used: {:.0f}s, remaining time: {:.0f}s)'.\n",
    "                  format(iteration, nrow_data, time_diff, time_remaining))\n",
    "            \n",
    "        df.loc[i, 'processed_text'] = text_processing(title, text)\n",
    "        \n",
    "    print('done')\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_corpus(df, set_seed):\n",
    "    nrow_data = len(df.index)\n",
    "    random.seed(set_seed)\n",
    "    sample_index = random.sample(range(nrow_data), nrow_data)\n",
    "    df = df.iloc[sample_index]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(df, data_path, data_set, set_seed=1, corpus_size=None):\n",
    "    \n",
    "    # resample corpus\n",
    "    print('resample corpus...' , end='')\n",
    "    df = resample_corpus(df, set_seed)\n",
    "    print('done')\n",
    "    \n",
    "    # cut corpus size (if used)\n",
    "    if corpus_size is not None:\n",
    "        df_cut = df.iloc[:corpus_size].copy()\n",
    "    else:\n",
    "        df_cut = df.copy()\n",
    "        \n",
    "    # start text processing\n",
    "    df_proc = restruct_text(df_cut)\n",
    "    \n",
    "    # keep only class and processed_text, \n",
    "    df_save = df_proc[['class', 'processed_text']].copy()\n",
    "        \n",
    "    # export, without duplicates\n",
    "    df_save.drop_duplicates(inplace=True)\n",
    "    df_save.to_csv('{}/{}'.format(data_path, data_set), index=False)\n",
    "\n",
    "    print('{} saved'.format(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_corpus(corpus, 'data', 'corpus.csv', 4, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
