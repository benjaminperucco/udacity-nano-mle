\documentclass[a4paper,12pt,nottoc]{article}
\usepackage{graphicx}
\usepackage[left = 3cm, right = 2cm, top = 2cm, bottom = 2cm]{geometry}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage[justification=centering]{caption}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tocbibind}
\usepackage{amsmath}

\begin{document}

\begin{center}{\LARGE Detecting Fake News with Supervised Learning}\\\vspace{.5cm}Benjamin R. Perucco\\\vspace{.25cm}December 30, 2020\end{center}

\section{Definition}

\subsection{Project Overview}

\subsection{Problem Statement}

\subsection{Metrics}

\section{Analysis}

\subsection{Algorithms and Techniques}

The news articles must be converted into a structured, mathematical representation in order to apply machine learning techniques. The following chapters disucss how we can convert text into numerical features.

\subsubsection{n-gram Model}

The $n$-gram defines usually a contiguous sequence of words with length $n$. For example, if $n = 1$, we speak of a unigram that contains only single word tokens. Or if $n = 2$, we denote this as a bigram that is build on two adjacent word tokens.\\

\noindent Consider the following text: ``Sometimes we eat green apples, and sometimes, the apples we eat are red.'' Based on a unigram, we obtain a set of tokens: $\{$'sometimes', `we', `eat', `apples', `green', `and', `the', `are', `red'$\}$. We can derive a frequency array of tokens in the text: [2, 2, 2, 2, 1, 1, 1, 1, 1]. For the bigram, another set of tokens is obtained: $\{$'sometimes we', `we eat', `eat green', `green apples', `apples and', `and sometimes', `sometimes the', `the apples', `apples we', `eat are', `are red'$\}$. The corresponding frequency array of tokens in the text is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\\

\noindent In order to build frequency arrays for a set of texts (or documents), we need to build a common vocabulary of which the $n$-gram model is underlying principle.

\subsubsection{Vocabulary}

Consider a corpus $D$ which contains a set of documents $\{d_1, d_2, d_3, ..., d_n\}$. Then a vocubulary $F$ is a set of tokens $\{f_1, f_2, f_3, ..., f_m\}$ extracted from the corpus $D$. Please remind yourself that a token is created on the n-gram model. Usually for a set of tokens, we consider the $m$ mostly occuring tokens in a corpus $D$. In the following, the tokens are denoted as features as these build the features (or independent variables) of a machine learning model.\\

\subsubsection{Definitions}

Let $\sigma(f_j, d_i)$ the number of occurances of feature $f_j$ in document $d_i$. Then we can build a feature matrix $X$, where

\begin{gather}
X =
\begin{bmatrix}
\sigma(f_1, d_1) & \sigma(f_2, d_1) & \sigma(f_3, d_1) & ... & \sigma(f_m, d_1) \\
\sigma(f_1, d_2) & \sigma(f_2, d_2) & \sigma(f_3, d_2) & ... & \sigma(f_m, d_2) \\
\sigma(f_1, d_3) & \sigma(f_2, d_3) & \sigma(f_3, d_3) & ... & \sigma(f_m, d_3) \\
... & ... & ... & ... & ... & \\
 \sigma(f_1, d_n) & \sigma(f_2, d_n) & \sigma(f_3, d_n) & ... & \sigma(f_m, d_n)
\end{bmatrix}.
\end{gather}

\noindent For a specific element in row $i$ and column $j$ (representing the document $d_i$ and feature $f_j$) in the matrix $X$, we abbreviate by using $\sigma_{ij}$.

\subsubsection{Term Frequency Model}

Matrix $X$ could be already used for machine learning, but in the term frequency model, matrix $X$ is normalized to matrix $\hat{X}$, where an element $\hat{\sigma}_{ij}$ is

\begin{gather}
\hat{\sigma}_{ij} = \frac{\sigma_{ij}}{\sum_{j=1}^{m}\sigma_{ij}}.
\end{gather}

\noindent Or spoken in plain language: the number of occurances of a token $f_j$ in a document $d_i$ is divided by the total number of occurances of all tokens $\{f_1, f_2, f_3, ..., f_m\}$ in the same document $d_i$. So we have a representation where the importance of each feature can be compared to other features in each document. 

\subsubsection{Inverse Document Frequency Model}

The inverse document frequency model is used to define the feature importance not just in a document $d_i$ but compared within a corpus $D$. We introduce a matrix $Y$, where an element $\delta_{ij}$ is 1 if $\hat{\sigma}_{ij} > 0$. An inverse normalization step is performed on the matrix $Y$ resulting in a vector $\hat{y}$, where an element $\hat{\delta}_{j}$ is

\begin{gather}\label{eq:idf}
\hat{\delta}_{j} = 1 + \textrm{log}\left[\frac{|D|}{\sum_{i=1}^{n}\delta_{ij}}\right].
\end{gather}

\noindent Or spoken in plain language: in a corpus $D$ which comprises of a set of documents $\{d_1, d_2, d_3, ..., d_n\}$, we count in how many documents the feature $f_j$ appears. This number is used to divide the number of documents $|D|$ in a corpus $D$. Consider two examples: if a feature $f_j$ occurs in each document $\{d_1, d_2, d_3, ..., d_n\}$, we divide the number of documents $|D|$ in a corpus $D$ by the same number. So expression \ref{eq:idf} results in $1$, weighting feature $f_j$ as $1$. On the other hand, if a feature $f_j$ occurs only in one document, expression \ref{eq:idf} produces a much larger number, increasing the weight of feature $f_j$ in the corpus $D$.

\subsection{Data Exploration}

\subsection{Benchmark}

\section{Methodology}

\subsection{Data Preprocessing}

\subsection{Implementation}

\subsection{Refinement}

\section{Results}

\subsection{Model Evaluation and Validation}

\subsection{Justification}

\section{Conclusion}

\subsection{Reflection}

\subsection{Improvement}



%\section*{Domain Background}
%
%Our world is highly interconnected and it is of paramount importance that citizens are informed objectively about issues that influence and shape our world (like issues on geopolitics or climate change). The internet lead to a rise of news media (e.g. social media, news portal, etc.) to report on these stories. The vast amount of (online) articles available yield a new phenomenon called fake news. Fake news is false or misleading information presented as news and can reduce the impact of real news \cite{bib:fakenews}.
%
%\section*{Problem Statement}
%
%How can fake news be distingiushed from reliable and trustworthy information? In the following capstone project, a machine learning model shall be developed whose aim is to detect fake news. The underlying problem is to bring words and sentences into a mathematical representation for machine learning to be applied.
%
%\section*{Datasets and Inputs}
%
%Articles classified as fake and real news are needed in order to train machine learning models. Such a dataset is taken from kaggle \cite{bib:kaggle} and was collected from real world sources. Truthful articles were obtained from Reuters and fake news articles were gathered from unreliable websites that were flagged by Politifact which is a fact-checking organization. These datasets contain different types of articles on different topics, the majority of articles focus on political and world news topics \cite{bib:isot}, \cite{bib:ahmed-2018}, \cite{bib:ahmed-2017}. There are about 21,417 real news articles and about 23,481 fake news articles in the dataset. So data is roughly balanced. Furthermore, the data consists of four columns: title, text, subject and date.
%
%\section*{Solution Statement}
%
%Natural language processing (NLP) models are used to bring words and sentences into a mathematical representation. Often applied is the n-gram model which is a contiguous sequence of items with length $n$ (such as words or characters) to be analyzed. Based on the n-gram model, features can be extracted as discussed by Hadeer et al. \cite{bib:ahmed-2018}:
% 
% \begin{itemize}
%\item{Term frequency (TF): TF defines the number of times a words appears in a document with respect to the number of total words in the document.}
%\item{Term frequency-inverted document frequency (TF-IDF): TF-IDF is a statistical metric used to measure how important a term is to a document compared to a set of documents.}
%\end{itemize}
%
%\section*{Benchmark Model}
%
%Hadeer et al. \cite{bib:ahmed-2017} achieved an accuracy of 92 \% when a linear support vector machine (LSVM) combined with 1-gram model and a 50,000 top feature selection TF-IDF method was used.
%
%\section*{Evaluation Metrics}
%
%The evaluation metric for this formulated problem is accuracy. This means how many labels are correctly classified (either as fake or real news) in respect to all classified labels.
%
%\section*{Project Design}
%
%Machine learning models shall be developed on AWS SageMaker \cite{bib:sagemaker}. The following project design is proposed:
%
%\begin{itemize}
%\item{Article exploration: The nature of the articles is investigated and a text processing strategy is derived.}
%\item{Text processing: Articles need to be processed with respect to formation, stop word removal, stemming, etc.}
%\item{Feature extraction: Dictionaries and n-gram models are built and together with techniques such as TF and TF-IDF features are extracted.}
%\item{Data preparation: Training, test and validation data for the machine learning models are prepared, for example 60 \% training,  20 \% test and 20 \% validation data.}
%\item{Modeling: Four machine learning models are evaluated sourcing from different frameworks such as support vector machine (SVM) from scikit-learn, linear learner (LL) and xgboost (XGB) from SageMaker built-in models and recurrent neural network (RNN) based on PyTorch.}
%\item{Tuning and validation: The test data is used to tune the machine learning hyperparameters and the validation data is used to evaluate the final metrics and to clarify the best modeling strategy.}
%\end{itemize}
%
%\begin{thebibliography}{9}
%\bibitem{bib:fakenews} Fake news, (2020, December 22), wikipedia.org, \\\texttt{https://en.wikipedia.org/wiki/Fake\_news}
%\bibitem{bib:kaggle} Fake and real news dataset, (2020, December 22), kaggle.com, \\\texttt{https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset}
%\bibitem{bib:isot} ISOT fake news detection datasets, (2020, December 22), University of Victoria, \texttt{https://www.uvic.ca/engineering/ece/isot/datasets/fake-news/index.php}
%\bibitem{bib:ahmed-2018} Ahmed, H, Traore, I, Saad, S. Detecting opinion spams and fake news using text classification, Security and Privacy, 2018, \texttt{https://doi.org/10.1001/spy2.9}
%\bibitem{bib:ahmed-2017} Ahmed H., Traore I., Saad S. (2017) Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham. \texttt{https://doi.org/10.1007/978-3-319-69155-8\_9}
%\bibitem{bib:sagemaker} Amazon SageMaker, (2020, December 22), amazon.com, \\\texttt{https://aws.amazon.com/sagemaker}
%\end{thebibliography}

\end{document}